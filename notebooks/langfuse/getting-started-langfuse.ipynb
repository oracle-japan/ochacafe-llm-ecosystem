{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03-getting-started-langfuse\n",
    "\n",
    "Langfuse の基本事項を試すためのノートブックです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 事前準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from operator import itemgetter\n",
    "\n",
    "from oci.auth.signers import InstancePrincipalsSecurityTokenSigner\n",
    "from oci.generative_ai_inference.generative_ai_inference_client import GenerativeAiInferenceClient\n",
    "from oci.generative_ai_inference.models import ChatDetails, OnDemandServingMode, CohereChatRequest\n",
    "\n",
    "from langchain_cohere import ChatCohere\n",
    "from langchain_community.llms.oci_generative_ai import OCIGenAI\n",
    "from langchain_community.chat_models.oci_generative_ai import ChatOCIGenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_community.llms.oci_generative_ai import OCIGenAI\n",
    "\n",
    "from langfuse import Langfuse\n",
    "from langfuse.callback import CallbackHandler\n",
    "from langfuse.decorators import observe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# Cohere\n",
    "api_key = os.getenv(\"COHERE_API_KEY\")\n",
    "\n",
    "# OCI\n",
    "compartment_id = os.getenv(\"COMPARTMENT_ID\")\n",
    "endpoint = os.getenv(\"GENAI_ENDPOINT\")\n",
    "\n",
    "# Langfuse\n",
    "secret_key = os.getenv(\"LANGFUSE_SECRET_KEY\")\n",
    "public_key = os.getenv(\"LANGFUSE_PUBLIC_KEY\")\n",
    "langfuse_host = os.getenv(\"LANGFUSE_HOST\")\n",
    "\n",
    "langfuse = Langfuse(\n",
    "    secret_key=secret_key,\n",
    "    public_key=public_key,\n",
    "    host=langfuse_host\n",
    ")\n",
    "langfuse_handler = CallbackHandler(\n",
    "    secret_key=secret_key,\n",
    "    public_key=public_key,\n",
    "    host=langfuse_host\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OCI Generative AI Service(OCI SDK for Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generative_ai_inference_client = GenerativeAiInferenceClient(\n",
    "    config={},\n",
    "    signer=InstancePrincipalsSecurityTokenSigner(),\n",
    "    service_endpoint=endpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Low-level SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def search(query: str) -> dict:\n",
    "    time.sleep(2)\n",
    "    return {\"text\": \"「Oracle Cloud Hangout Cafe」(通称「おちゃかふぇ」/以降、OCHaCafe)は、日本オラクルが主催するコミュニティの1つです。定期的に、開発者・エンジニアに向けたクラウドネイティブな時代に身につけておくべきテクノロジーを深堀する勉強会を開催しています。\"}\n",
    "\n",
    "def generate_text_with_low_level_sdk(query: str, **kwargs):\n",
    "    trace = langfuse.trace(\n",
    "        name = \"tracing example\"\n",
    "    )\n",
    "    span = trace.span(\n",
    "        name = \"embedding-search\",\n",
    "        metadata={\"database\": \"mock database\"},\n",
    "        input={\"query\": query}\n",
    "    )\n",
    "    document = search(query)\n",
    "    span.end(output=document)\n",
    "    generation = trace.generation(\n",
    "        name=\"Text Generation\",\n",
    "        model=\"cohere.command-r-16k\",\n",
    "        model_parameters={\"maxTokens\": \"512\", \"temperature\": \"0.75\", \"documents\": [document[\"text\"]]},\n",
    "        input=[{\"role\": \"system\", \"content\": \"あなたは有能なアシスタントです\"}, {\"role\": \"user\", \"content\": query}]\n",
    "    )\n",
    "    response = generative_ai_inference_client.chat(\n",
    "        chat_details=ChatDetails(\n",
    "            compartment_id=compartment_id,\n",
    "            serving_mode=OnDemandServingMode(\n",
    "                model_id=\"cohere.command-r-16k\"\n",
    "            ),\n",
    "            chat_request=CohereChatRequest(\n",
    "                message=query,\n",
    "                max_tokens=512,\n",
    "                temperature=0.75,\n",
    "                documents=[document]\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    generation.end(output=response.data.chat_response.text)\n",
    "\n",
    "generate_text_with_low_level_sdk(\"OCHaCafeってなんですか？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python Decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@observe()\n",
    "def search_with_decorator(query: str) -> dict:\n",
    "    time.sleep(2)\n",
    "    return {\"text\": \"「Oracle Cloud Hangout Cafe」(通称「おちゃかふぇ」/以降、OCHaCafe)は、日本オラクルが主催するコミュニティの1つです。定期的に、開発者・エンジニアに向けたクラウドネイティブな時代に身につけておくべきテクノロジーを深堀する勉強会を開催しています。\"}\n",
    "\n",
    "@observe(as_type=\"generation\")\n",
    "def generate_text_with_decorator(query: str, **kwargs):\n",
    "    document = search_with_decorator(query=query)\n",
    "    res = generative_ai_inference_client.chat(\n",
    "        chat_details=ChatDetails(\n",
    "            compartment_id=compartment_id,\n",
    "            serving_mode=OnDemandServingMode(\n",
    "                model_id=\"cohere.command-r-16k\"\n",
    "            ),\n",
    "            chat_request=CohereChatRequest(\n",
    "                message=query,\n",
    "                max_tokens=512,\n",
    "                documents=[document]\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return res.data\n",
    "\n",
    "res = generate_text_with_decorator(\"OCHaCafeってなんですか？\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OCI Generative AI Service(LangChain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_ai = OCIGenAI(\n",
    "    auth_type=\"API_KEY\",\n",
    "    auth_profile=\"CHICAGO\",\n",
    "    model_id=\"cohere.command\",\n",
    "    compartment_id=compartment_id,\n",
    "    model_kwargs={\n",
    "        \"max_tokens\": 200\n",
    "    }\n",
    ")\n",
    "\n",
    "res = gen_ai.invoke(\"OCHaCafeってなんですか？\", config={\"callbacks\": [langfuse_handler]})\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatCohere(\n",
    "    model=\"command-r-plus\",\n",
    "    cohere_api_key=api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langfuse_handler = CallbackHandler(\n",
    "    secret_key=secret_key,\n",
    "    public_key=public_key,\n",
    "    host=langfuse_host\n",
    ")\n",
    "chat = ChatCohere(\n",
    "    model=\"command-r-plus\",\n",
    "    cohere_api_key=api_key\n",
    ")\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(\"{person}はどの街出身ですか？\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\"{city}はどの国ですか？この質問を{language}で答えてください。\")\n",
    "\n",
    "chain1 = prompt1 | chat | StrOutputParser()\n",
    "chain2 = (\n",
    "    {\"city\": chain1, \"language\": itemgetter(\"language\")}\n",
    "    | prompt2\n",
    "    | chat\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain2.invoke({\"person\": \"徳永家康\", \"language\": \"スペイン語\"}, config={\"callbacks\": [langfuse_handler]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langfuse = Langfuse(\n",
    "    secret_key=secret_key,\n",
    "    public_key=public_key,\n",
    "    host=langfuse_host\n",
    ")\n",
    "\n",
    "chat = ChatCohere(\n",
    "    model=\"command-r-plus\",\n",
    "    cohere_api_key=api_key,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "chat_prompt = langfuse.get_prompt(name=\"simple-chat-prompt\", type=\"chat\")\n",
    "# print(chat_prompt.compile(person=\"徳永家康\"))\n",
    "chat.invoke(chat_prompt.compile(person=\"徳永家康\"), config={\"callbacks\": [langfuse_handler]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langfuse = Langfuse(\n",
    "    secret_key=secret_key,\n",
    "    public_key=public_key,\n",
    "    host=langfuse_host\n",
    ")\n",
    "\n",
    "chat = ChatCohere(\n",
    "    model=\"command-r-plus\",\n",
    "    cohere_api_key=api_key,\n",
    "    metadata={\n",
    "        \"max_tokens\": 1024\n",
    "    }\n",
    ")\n",
    "\n",
    "chat_prompt = langfuse.get_prompt(name=\"recipe-chat-prompt\", type=\"chat\")\n",
    "print(chat_prompt.compile(name=\"カレー\"))\n",
    "\n",
    "chat.invoke(chat_prompt.compile(name=\"カレー\"), config={\"callbacks\": [langfuse_handler]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langfuse = Langfuse(\n",
    "    secret_key=secret_key,\n",
    "    public_key=public_key,\n",
    "    host=langfuse_host\n",
    ")\n",
    "\n",
    "chat = ChatCohere(\n",
    "    model=\"command-r-plus\",\n",
    "    cohere_api_key=api_key,\n",
    "    metadata={\n",
    "        \"max_tokens\": 1024\n",
    "    }\n",
    ")\n",
    "\n",
    "chat_prompt = langfuse.get_prompt(name=\"recipe-chat-prompt\", type=\"chat\")\n",
    "print(chat_prompt.compile(name=\"カレー\"))\n",
    "\n",
    "chat.invoke(chat_prompt.compile(name=\"カレー\"), config={\"callbacks\": [langfuse_handler]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_id = langfuse_handler.get_trace_id()\n",
    "print(\"trace_id\", trace_id)\n",
    "trace = langfuse.score(\n",
    "    trace_id=trace_id,\n",
    "    value=1,\n",
    "    name=\"user-feedback\",\n",
    "    comment=\"美味しそうなのでOKです\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatCohere(model=\"command-r-plus\", cohere_api_key=api_key)\n",
    "\n",
    "messages = [HumanMessage(content=\"OCHaCafeとはなんですか？\")]\n",
    "res = chat.invoke(messages, config={\"callbacks\": [langfuse_handler]})\n",
    "\n",
    "langfuse = Langfuse(\n",
    "    secret_key=secret_key,\n",
    "    public_key=public_key,\n",
    "    host=langfuse_host\n",
    ")\n",
    "langfuse_handler = CallbackHandler(\n",
    "    secret_key=secret_key,\n",
    "    public_key=public_key,\n",
    "    host=langfuse_host\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_id = langfuse_handler.get_trace_id()\n",
    "print(\"trace_id\", trace_id)\n",
    "trace = langfuse.score(\n",
    "    trace_id=trace_id,\n",
    "    value=0,\n",
    "    name=\"user-feedback\",\n",
    "    comment=\"OCHaCafeは、お茶をテーマにしたユニークなカフェではありません。\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_TYPES={\n",
    "    \"hallucination\": True,\n",
    "    \"conciseness\": True,\n",
    "    \"relevance\": True,\n",
    "    \"coherence\": True,\n",
    "    \"harmfulness\": True,\n",
    "    \"maliciousness\": True,\n",
    "    \"helpfulness\": True,\n",
    "    \"controversiality\": True,\n",
    "    \"misogyny\": True,\n",
    "    \"criminality\": True,\n",
    "    \"insensitivity\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "from langchain_community.llms.oci_generative_ai import OCIGenAI\n",
    "\n",
    "def get_evaluator_for_key(key: str):\n",
    "    llm = ChatOCIGenAI(\n",
    "        auth_type=\"INSTANCE_PRINCIPAL\",\n",
    "        model_id=\"cohere.command-r-plus\",\n",
    "        compartment_id=compartment_id,\n",
    "        service_endpoint=endpoint,\n",
    "        is_stream=False,\n",
    "        model_kwargs={\n",
    "            \"max_tokens\": 500,\n",
    "            \"temperature\": 0,\n",
    "            \"top_k\": 0,\n",
    "            \"top_p\": 0.75,\n",
    "            \"frequency_penalty\": 0,\n",
    "            \"presence_penalty\": 0,\n",
    "        }\n",
    "    )\n",
    "    return load_evaluator(\"criteria\", criteria=key, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation.criteria import LabeledCriteriaEvalChain\n",
    "\n",
    "def get_hallucination_eval():\n",
    "    criteria = {\n",
    "        \"hallucination\": (\n",
    "            \"Does this submission contain information\"\n",
    "            \"not present in the input or reference?\"\n",
    "        ),\n",
    "    }\n",
    "    llm = ChatOCIGenAI(\n",
    "        auth_type=\"INSTANCE_PRINCIPAL\",\n",
    "        model_id=\"cohere.command-r-plus\",\n",
    "        compartment_id=compartment_id,\n",
    "        service_endpoint=endpoint,\n",
    "        is_stream=False,\n",
    "        model_kwargs={\n",
    "            \"max_tokens\": 500,\n",
    "            \"temperature\": 0,\n",
    "            \"top_k\": 0,\n",
    "            \"top_p\": 0.75,\n",
    "            \"frequency_penalty\": 0,\n",
    "            \"presence_penalty\": 0,\n",
    "        }\n",
    "    )\n",
    "    return LabeledCriteriaEvalChain.from_llm(\n",
    "        llm=llm,\n",
    "        criteria=criteria,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_all_pages(name=None, user_id = None, limit=50):\n",
    "    page = 1\n",
    "    all_data = []\n",
    " \n",
    "    while True:\n",
    "        response = langfuse.get_generations(name=name, limit=limit, user_id=user_id, page=page)\n",
    "        if not response.data:\n",
    "            break\n",
    " \n",
    "        all_data.extend(response.data)\n",
    "        page += 1\n",
    " \n",
    "    return all_data\n",
    "\n",
    "generations = fetch_all_pages(name=\"ChatOCIGenAI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_eval_and_score():\n",
    "  for generation in generations:\n",
    "    criteria = [key for key, value in EVAL_TYPES.items() if value and key != \"hallucination\"]\n",
    "    for criterion in criteria:\n",
    "      eval_result = get_evaluator_for_key(criterion).evaluate_strings(\n",
    "          prediction=generation.output,\n",
    "          input=generation.input,\n",
    "      )\n",
    "      print(eval_result)\n",
    "      langfuse.score(name=criterion, trace_id=generation.trace_id, observation_id=generation.id, value=eval_result[\"score\"], comment=eval_result['reasoning'])\n",
    " \n",
    "execute_eval_and_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = langfuse.get_dataset(name=\"ochat-demo-dataset\")\n",
    "for item in dataset.items:\n",
    "    handler = item.get_langchain_handler(run_name=\"ochat-test\")\n",
    "    chat.invoke(input=item.input, config={\"callbacks\": [handler]})\n",
    "langfuse.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import oci\n",
    "\n",
    "client = oci.object_storage.ObjectStorageClient()\n",
    "client.create_bucket"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
