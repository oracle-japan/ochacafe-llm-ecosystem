{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_community.chat_models.oci_generative_ai import ChatOCIGenAI\n",
    "from langfuse.callback import CallbackHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テキスト生成 - ストリーミング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = load_dotenv()\n",
    "compartment_id = os.getenv(\"COMPARTMENT_ID\")\n",
    "service_endpoint = os.getenv(\"GENAI_ENDPOINT\")\n",
    "# Langfuse\n",
    "secret_key = os.getenv(\"LANGFUSE_SECRET_KEY\")\n",
    "public_key = os.getenv(\"LANGFUSE_PUBLIC_KEY\")\n",
    "langfuse_host = os.getenv(\"LANGFUSE_HOST\")\n",
    "\n",
    "print(\"servce endpoint: \", service_endpoint)\n",
    "print(\"compartment id: \", compartment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOCIGenAI(\n",
    "    auth_type=\"INSTANCE_PRINCIPAL\",\n",
    "    service_endpoint=service_endpoint,\n",
    "    compartment_id=compartment_id,\n",
    "    model_id=\"cohere.command-r-plus\",\n",
    "    is_stream=True,\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0,\n",
    "        \"max_tokens\": 500,\n",
    "        \"top_p\": 0.75,\n",
    "        \"top_k\": 0,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"presence_penalty\": 0\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langfuse_handler = CallbackHandler(\n",
    "    secret_key=secret_key,\n",
    "    public_key=public_key,\n",
    "    host=langfuse_host\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = chat.stream(\n",
    "    \"OCHaCafeってなんですか？\",\n",
    "    config={\"callbacks\": [langfuse_handler]},\n",
    ")\n",
    "\n",
    "for chunk in res:\n",
    "    print(chunk.__dict__)\n",
    "    print(chunk.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "エンベディング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.oci_generative_ai import OCIGenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OCIGenAIEmbeddings(\n",
    "    auth_type=\"INSTANCE_PRINCIPAL\",\n",
    "    model_id=\"cohere.embed-multilingual-v3.0\",\n",
    "    service_endpoint=service_endpoint,\n",
    "    compartment_id=compartment_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"「Oracle Cloud Hangout Cafe」(通称「おちゃかふぇ」/以降、OCHaCafe)は、日本オラクルが主催するコミュニティの1つです。定期的に、開発者・エンジニアに向けたクラウドネイティブな時代に身につけておくべきテクノロジーを深堀する勉強会を開催しています。\",\n",
    "    \"日本オラクル株式会社（にほんオラクル、英: Oracle Corporation Japan）は、米国企業オラクルコーポレーション (Oracle Corporation) （1977年設立）が、1985年に日本で設立した法人である。\"\n",
    "]\n",
    "\n",
    "res = embeddings.embed_documents(docs)\n",
    "\n",
    "print(\"Dims: \", len(res[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_milvus import Milvus\n",
    "from langchain_community.chat_models.oci_generative_ai import ChatOCIGenAI\n",
    "from langchain_community.embeddings.oci_generative_ai import OCIGenAIEmbeddings\n",
    "\n",
    "class OciGenerativeAi:\n",
    "    def __init__(self, compartment_id: str, service_endpoint: str, **kwargs):\n",
    "        self.compartment_id = compartment_id\n",
    "        self.service_endpoint = service_endpoint\n",
    "        self.chat_model = self._initialize_chat_model(\n",
    "            is_stream=kwargs.get(\"streaming\"),\n",
    "            max_tokens=kwargs.get(\"max_tokens\"),\n",
    "            temperature=kwargs.get(\"temperature\"),\n",
    "            top_k=kwargs.get(\"top_k\"),\n",
    "            top_p=kwargs.get(\"top_p\"),\n",
    "            frequency_penalty=kwargs.get(\"frequency_penalty\"),\n",
    "            presence_penalty=kwargs.get(\"presence_penalty\"),\n",
    "\n",
    "        )\n",
    "        self.embeddings_model = self._initialize_embedding_model()\n",
    "        self.callback_handler = kwargs.get(\"callback_handler\")\n",
    "        if (\"milvus_uri\" in kwargs) & (\"collection_name\" in kwargs):\n",
    "            self.milvus = Milvus(\n",
    "                embedding_function=self.embeddings_model,\n",
    "                collection_name=kwargs.get(\"collection_name\"),\n",
    "                connection_args={\"uri\": kwargs.get(\"milvus_uri\")}\n",
    "            )\n",
    "            self.retriever = self.milvus.as_retriever(\n",
    "                search_type=kwargs.get(\"search_type\"),\n",
    "                search_kwargs={\n",
    "                    \"k\": kwargs.get(\"return_k\"),\n",
    "                    \"score_threshold\": kwargs.get(\"score_threshold\"),\n",
    "                    \"fetch_k\": kwargs.get(\"fetch_k\"),\n",
    "                    \"lambda_mult\": kwargs.get(\"lambda_mult\")\n",
    "                }\n",
    "            )\n",
    "\n",
    "    def chat(self, input: str, streaming: bool):\n",
    "        if streaming == True:\n",
    "            response = self.chat_model.stream(\n",
    "                input,\n",
    "                config={\"callbacks\": [self.callback_handler], \"configurable\": {\"session_id\": st.session_state[\"session_id\"]}},\n",
    "            )\n",
    "            for chunk in response:\n",
    "                yield chunk.content\n",
    "        else:\n",
    "            response = self.chat_model.invoke(\n",
    "                input,\n",
    "                config={\"callbacks\": [self.callback_handler], \"configurable\": {\"session_id\": st.session_state[\"session_id\"]}},\n",
    "            )\n",
    "            return response\n",
    "\n",
    "    def chat_with_rag(self, input: str, streaming: bool):\n",
    "        langfuse = st.session_state[\"langfuse\"]\n",
    "        prompt_template = PromptTemplate.from_template(\n",
    "            langfuse.get_prompt(name=\"demo-user-prompt\", type=\"text\").prompt,\n",
    "            template_format=\"jinja2\"\n",
    "        )\n",
    "        chain = (\n",
    "            {\"context\": self.retriever, \"question\": RunnablePassthrough()}\n",
    "            | prompt_template\n",
    "            | self.chat_model\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        if streaming == True:\n",
    "            response = chain.stream(\n",
    "                input,\n",
    "                config={\"callbacks\": [self.callback_handler], \"configurable\": {\"session_id\": st.session_state[\"session_id\"]}},\n",
    "            )\n",
    "            for chunk in response:\n",
    "                yield chunk.content\n",
    "        else:\n",
    "            response = chain.invoke(\n",
    "                input,\n",
    "                config={\"callbacks\": [self.callback_handler], \"configurable\": {\"session_id\": st.session_state[\"session_id\"]}},\n",
    "            )\n",
    "            return response\n",
    "\n",
    "    def _initialize_chat_model(self, is_stream: bool, max_tokens: int, temperature: float, top_k: int, top_p: float, frequency_penalty: float, presence_penalty: float) -> ChatOCIGenAI:\n",
    "        return ChatOCIGenAI(\n",
    "            auth_type=\"INSTANCE_PRINCIPAL\",\n",
    "            model_id=\"cohere.command-r-plus\",\n",
    "            compartment_id=self.compartment_id,\n",
    "            service_endpoint=self.service_endpoint,\n",
    "            is_stream=is_stream,\n",
    "            model_kwargs={\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"temperature\": temperature,\n",
    "                \"top_k\": top_k,\n",
    "                \"top_p\": top_p,\n",
    "                \"frequency_penalty\": frequency_penalty,\n",
    "                \"presence_penalty\": presence_penalty,\n",
    "            },\n",
    "            metadata={\n",
    "                \"model_name\": \"cohere.command-r-plus\",\n",
    "                \"model_parameters\": {\n",
    "                    \"is_stream\": is_stream,\n",
    "                    \"max_tokens\": max_tokens,\n",
    "                    \"temperature\": temperature,\n",
    "                    \"top_k\": top_k,\n",
    "                    \"top_p\": top_p,\n",
    "                    \"frequency_penalty\": frequency_penalty,\n",
    "                    \"presence_penalty\": presence_penalty,\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def _initialize_embedding_model(self) -> OCIGenAIEmbeddings:\n",
    "        return OCIGenAIEmbeddings(\n",
    "            auth_type=\"INSTANCE_PRINCIPAL\",\n",
    "            compartment_id=self.compartment_id,\n",
    "            service_endpoint=self.service_endpoint,\n",
    "            model_id=\"cohere.embed-multilingual-v3.0\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohere = OciGenerativeAi(\n",
    "    compartment_id=compartment_id,\n",
    "    service_endpoint=service_endpoint,\n",
    "    streaming=True,\n",
    "    max_tokens=500,\n",
    "    temperature=0,\n",
    "    k=0,\n",
    "    p=0.75,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = cohere.chat(\n",
    "    input=\"OCHaCafeってなんですか？\",\n",
    "    streaming=True,\n",
    ")\n",
    "\n",
    "message = \"\"\n",
    "for content in response:\n",
    "    message += content\n",
    "    print(content, end=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
